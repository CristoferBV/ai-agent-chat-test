# ğŸ¤– Punta Blanca RAG Agent

Este proyecto implementa un **agente RAG (Retrieval-Augmented Generation)** usando **Google Gemini**, **LangChain**, **FAISS**, y **FastAPI**.  
Su propÃ³sito es responder preguntas sobre **Punta Blanca Solutions**, apoyÃ¡ndose en informaciÃ³n extraÃ­da de su sitio web y de fuentes pÃºblicas.

---

## âœ¨ CaracterÃ­sticas

- ğŸ” **Retrieval**: bÃºsqueda semÃ¡ntica sobre documentos indexados con FAISS.
- ğŸ“‘ **Fuentes de conocimiento**:
  - Web scraping de [`puntablanca.ai`](https://www.puntablanca.ai/)
  - Archivos locales como `linkedin.md` (y opcionalmente `facts.md`).
- ğŸŒ **API REST con FastAPI** con documentaciÃ³n interactiva en `/docs`.
- ğŸ§  **Modelo generativo**: **Google Gemini 2.0 Flash** con respuestas estrictamente en JSON.
- ğŸ“¦ **ContenerizaciÃ³n con Docker**.
- â˜ï¸ **Despliegue previsto en Google Cloud Run**.

---

## ğŸ“‚ Estructura del proyecto

```
ai-agent-chat-test/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/                # LÃ³gica de la API
â”‚   â”‚   â”œâ”€â”€ main.py         # Entrada FastAPI
â”‚   â”‚   â”œâ”€â”€ graph.py        # Flujo RAG con LangGraph
â”‚   â”‚   â”œâ”€â”€ retrieval.py    # RecuperaciÃ³n de contexto FAISS
â”‚   â”‚   â”œâ”€â”€ generation.py   # ConexiÃ³n con Gemini
â”‚   â”‚   â””â”€â”€ schemas.py      # Modelos Pydantic
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ sources/        # Archivos locales (linkedin.md, facts.md, etc.)
â”‚   â”‚   â””â”€â”€ vectorstore/    # Ãndice FAISS generado
â”‚   â”‚
â”‚   â”œâ”€â”€ ingest/             # Scripts de ingestiÃ³n/utilidades
â”‚   â”‚   â”œâ”€â”€ build_vectorstore.py
â”‚   â”‚   â”œâ”€â”€ inspect_vectorstore.py
â”‚   â”‚   â””â”€â”€ search_probe.py
â”‚   â”‚
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ docs/                   # DocumentaciÃ³n extendida
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ API.md
â””â”€â”€ ARCHITECTURE.md
```

---

## ğŸš€ EjecuciÃ³n local

### 1. Clonar el repositorio
```bash
git clone https://github.com/usuario/ai-agent-test.git
cd ai-agent-test
```

### 2. Instalar dependencias
```bash
python -m venv .venv
# Linux/Mac
source .venv/bin/activate
# Windows PowerShell
.venv\Scripts\activate

pip install -r requirements.txt
```

### 3. Configurar `.env`
Crea `./backend/.env` (o en la raÃ­z) con:

```env
GEMINI_API_KEY=tu_api_key_aqui
GEMINI_MODEL=gemini-2.0-flash
```

### 4. Construir el vectorstore
```bash
python backend/ingest/build_vectorstore.py
```

### 5. Levantar FastAPI
```bash
uvicorn app.main:app --app-dir backend --host 0.0.0.0 --port 8080
```
Abrir: http://localhost:8080/docs

---

## ğŸ“¡ Ejemplo de consulta

```bash
curl -X POST "http://localhost:8080/api/ask"   -H "Content-Type: application/json"   -d '{"question": "Â¿QuÃ© servicios ofrece Punta Blanca?"}'
```

**Respuesta esperada** (ejemplo):
```json
{
  "answer": "Punta Blanca ofrece soluciones de IA y estrategias de transformaciÃ³n digital...",
  "sources": [
    "https://www.puntablanca.ai/services",
    "file://linkedin.md"
  ],
  "confidence": 0.9
}
```

---

## ğŸ³ ContenerizaciÃ³n

```bash
docker build -t ai-agent-chat-test .
docker run -p 8080:8080 ai-agent-chat-test
```

---

## â˜ï¸ Despliegue en Cloud Run (resumen)

```bash
gcloud builds submit --tag gcr.io/<PROJECT_ID>/ai-agent
gcloud run deploy ai-agent   --image gcr.io/<PROJECT_ID>/ai-agent   --platform managed   --region us-central1   --allow-unauthenticated
```

---

## ğŸ‘¨â€ğŸ’» Autor

Desarrollado como **prueba tÃ©cnica** para integrar **IA generativa + RAG** con despliegue en **Google Cloud**.
