# ðŸ—ï¸ Arquitectura â€“ Punta Blanca RAG Agent

---

## ðŸ”¹ Flujo General (RAG)

1. **Usuario** envÃ­a una pregunta vÃ­a `/api/ask`.
2. **Retriever** busca chunks relevantes en el Ã­ndice **FAISS**.
3. Se construye un **contexto** con fragmentos de texto.
4. Se pasa la pregunta + contexto al modelo **Google Gemini**.
5. Gemini devuelve un JSON con:
   - `answer`
   - `sources`
   - `confidence`
6. La API responde en formato estructurado.

---

## ðŸ”¹ Componentes

- **FastAPI**  
  - Define endpoints (`/api/ask`, `/healthz`).  
  - Usa **Pydantic** (`AskRequest`, `AskResponse`) para validaciÃ³n.

- **Retriever (retrieval.py)**  
  - Carga embeddings multilingÃ¼es:  
    `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`  
  - Indexa y consulta con **FAISS** (similarity/MMR).

- **Ingest (build_vectorstore.py)**  
  - Descarga contenido desde:
    - `puntablanca.ai`
    - Archivos locales (`linkedin.md`, opcional `facts.md`)
  - Limpia texto (Trafilatura).
  - Divide en **chunks** (~800 caracteres, overlap ~120).
  - Construye Ã­ndice FAISS persistente.

- **Generator (generation.py)**  
  - Inicializa modelo **Gemini** (p. ej., `gemini-2.0-flash`).
  - Usa `system_instruction` para forzar salida **JSON**.  
  - Normaliza/asegura formato ante respuestas no vÃ¡lidas.

- **LangGraph (graph.py)**  
  - Define flujo tipo grafo: **input â†’ retrieval â†’ generation â†’ output**.

---

## ðŸ”¹ Diagrama Simplificado

```mermaid
flowchart TD
    User[Usuario pregunta] --> API[FastAPI /api/ask]
    API --> Retriever[Retriever FAISS]
    Retriever --> Contexto[Contexto + Fuentes]
    Contexto --> Gemini[Google Gemini]
    Gemini --> JSON[Respuesta JSON]
    JSON --> API
    API --> User
```

---

## ðŸ”¹ Despliegue

- **Local** con Uvicorn.  
- **Docker** â†’ Imagen reproducible.  
- **Google Cloud Run** â†’ Despliegue sin servidor.